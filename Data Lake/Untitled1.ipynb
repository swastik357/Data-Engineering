{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType as StructType, DoubleType as DoubleType, StructField as StructField\n",
    "from pyspark.sql.types import StringType as StringType, IntegerType as IntegerType, TimestampType\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['KEYS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['KEYS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "#output_data = \"arn:aws:s3:::sparkify-mybucket\"\n",
    "output_data = \"s3a://sparkify-mybucket/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_data = input_data + 'song_data/A/B/C/*.json'\n",
    "    \n",
    "songSchema = StructType([\n",
    "    StructField(\"song_id\",StringType()),\n",
    "    StructField(\"artist_id\",StringType()),\n",
    "    StructField(\"artist_latitude\",DoubleType()),\n",
    "    StructField(\"artist_location\",StringType()),\n",
    "    StructField(\"artist_longitude\",DoubleType()),\n",
    "    StructField(\"artist_name\",StringType()),\n",
    "    StructField(\"duration\",DoubleType()),\n",
    "    StructField(\"num_songs\",IntegerType()),\n",
    "    StructField(\"title\",StringType()),\n",
    "    StructField(\"year\",IntegerType()),\n",
    "    ])\n",
    "    \n",
    "    # read song data file\n",
    "df = spark.read.json(song_data, schema=songSchema)\n",
    "df.createOrReplaceTempView(\"song_data_view\")\n",
    "    # extract columns to create songs table \n",
    "song_fields = [\"title\", \"artist_id\",\"year\", \"duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songs_table = df.select(song_fields).dropDuplicates() \\\n",
    ".withColumn(\"song_id\", monotonically_increasing_id())\n",
    "# Write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.mode(\"overwrite\").partitionBy('year', 'artist_id')\\\n",
    ".parquet(output_data + 'songs/')\n",
    "\n",
    "# extract columns to create artists table\n",
    "artists_table = df.select(\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\") \\\n",
    ".dropDuplicates()\n",
    "    \n",
    "# write artists table to parquet files\n",
    "artists_table.write.mode('overwrite').parquet(output_data+'artists/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "log_data = input_data + 'log_data/*/*/*.json'\n",
    "\n",
    "    # read log data file\n",
    "df = spark.read.json(log_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "df = df.filter(df.page == 'NextSong')\n",
    "df.createOrReplaceTempView(\"log_data_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "users_table = df.select(\"userId\", \"firstName\", \"lastName\", \"gender\", \"level\").dropDuplicates()\n",
    "users_table.write.mode(\"overwrite\").parquet(output_data + 'users/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x : datetime.utcfromtimestamp(int(x) / 1000), TimestampType())\n",
    "df = df.withColumn(\"start_time\", get_timestamp(\"ts\"))\n",
    "    \n",
    "# extract columns to create time table \n",
    "df = df.withColumn(\"hour\", hour(\"start_time\")) \n",
    "df = df.withColumn(\"day\", dayofmonth(\"start_time\")) \n",
    "df = df.withColumn(\"week\", weekofyear(\"start_time\"))\n",
    "df = df.withColumn(\"month\", month(\"start_time\"))\n",
    "df = df.withColumn(\"year\", year(\"start_time\"))\n",
    "df = df.withColumn(\"weekday\", date_format(col(\"start_time\"), 'E'))\n",
    "    \n",
    "time_table = df.select(\"start_time\", \"hour\", \"day\", \"week\", \"month\",\\\n",
    "                           \"year\", \"weekday\").drop_duplicates()\n",
    "    \n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.mode('overwrite').partitionBy(\"year\", \"month\")\\\n",
    ".parquet(output_data+'time/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[song_id: string, artist_id: string, artist_latitude: double, artist_location: string, artist_longitude: double, artist_name: string, duration: double, num_songs: int, title: string, year: int]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from song_data_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df_songs = spark.read.parquet(output_data+'songs/')\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = spark.sql(\"\"\"\n",
    "                            SELECT monotonically_increasing_id() as songplay_id,\n",
    "                            to_timestamp(logTable.ts/1000) as start_time,\n",
    "                            logTable.userId as user_id,\n",
    "                            logTable.level as level,\n",
    "                            songTable.song_id as song_id,\n",
    "                            songTable.artist_id as artist_id,\n",
    "                            logTable.sessionId as session_id,\n",
    "                            logTable.location as location,\n",
    "                            logTable.userAgent as user_agent,\n",
    "                            month(to_timestamp(logTable.ts/1000)) as month,\n",
    "                            year(to_timestamp(logTable.ts/1000)) as year\n",
    "                            FROM log_data_view logTable\n",
    "                            JOIN song_data_view songTable on logTable.artist = songTable.artist_name and \n",
    "                            logTable.song = songTable.title\n",
    "                        \"\"\")\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.mode('overwrite').partitionBy(\"year\", \"month\").parquet(output_data+'songplays/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
